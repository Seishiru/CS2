{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8f332f2",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#f0f0f0; padding:20px; border-radius:12px; max-width:1490px; width:100%; box-sizing:border-box;\">\n",
    "\n",
    "  <h1 style=\"margin:0; color:#2c3e50; font-size:32px;\">001 🏠 Cleaning Setup</h1>\n",
    "\n",
    "  <p style=\"margin:5px 0 0 0; font-size:16px; color:#34495e;\">\n",
    "    <strong>Authors:</strong> Cecil Quibranza & Matthew Israel &nbsp;&nbsp;|&nbsp;&nbsp; \n",
    "    <strong>Date:</strong> 2025-08-30\n",
    "  </p>\n",
    "\n",
    "  <hr style=\"margin:10px 0; border:none; border-top:2px solid #dcdcdc;\">\n",
    "\n",
    "  <p style=\"margin:0; font-size:16px; color:#34495e;\">\n",
    "    This is the <strong>first notebook</strong>, focusing on the <strong>cleaning of images</strong>. \n",
    "    The goal is to standardize and prepare datasets by removing redundant or irrelevant data, ensuring that only high-quality, non-augmented images are retained. \n",
    "    This process minimizes dataset noise and prevents biases during training.\n",
    "  </p>\n",
    "\n",
    "  <ol style=\"margin:10px 0 0 20px; color:#34495e; font-size:16px; list-style-position:inside;\">\n",
    "    <li><strong>Extract:</strong> Unzip the dataset files into their respective folders to make all images accessible for preprocessing.</li>\n",
    "    <li><strong>Remove augmented/duplicates:</strong> Manually remove rotated/mirrored/heavily filtered images and exact duplicates. \n",
    "        Keep only originals to reduce imbalance and overfitting.</li>\n",
    "    <li><strong>Move cleaned images:</strong> Consolidate remaining images into a single folder structure by class (e.g., disease/condition). \n",
    "        This enforces a uniform layout for loading and evaluation.</li>\n",
    "    <li><strong>Repeat for all sources:</strong> Apply steps 2–3 across every dataset to maintain consistency for downstream training.</li>\n",
    "  </ol>\n",
    "\n",
    "  <p style=\"margin:10px 0 0 0; font-size:15px; color:#7f8c8d;\">\n",
    "    ✅ This establishes a clean baseline so later steps (resize, normalization, controlled augmentation) operate on unbiased data.\n",
    "  </p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4ba70f",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#f0f0f0; padding:20px; border-radius:12px; max-width:1490px; width:100%; box-sizing:border-box;\">\n",
    "\n",
    "  <h1 style=\"margin:0; color:#2c3e50; font-size:32px;\">Modules</h1>\n",
    "\n",
    "  <p style=\"margin:10px 0 0 0; font-size:16px; color:#34495e;\">\n",
    "    The <strong>code</strong> below installs necessary python modules for this project\n",
    "  </p>\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5321a8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting termcolor\n",
      "  Using cached termcolor-3.1.0-py3-none-any.whl.metadata (6.4 kB)\n",
      "Using cached termcolor-3.1.0-py3-none-any.whl (7.7 kB)\n",
      "Installing collected packages: termcolor\n",
      "Successfully installed termcolor-3.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find platform independent libraries <prefix>\n"
     ]
    }
   ],
   "source": [
    "%pip install termcolor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08dfaf95",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#f0f0f0; padding:20px; border-radius:12px; max-width:1490px; width:100%; box-sizing:border-box;\">\n",
    "\n",
    "  <h1 style=\"margin:0; color:#2c3e50; font-size:32px;\">1. Extract</h1>\n",
    "\n",
    "  <p style=\"margin:10px 0 0 0; font-size:16px; color:#34495e;\">\n",
    "    The <strong>code</strong> below unzips files in the \"RawDatasets\" folder and place them in a new folder called \"Extracted\".\n",
    "  </p>\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3286e685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted: Acne_0.zip → Extracted\\Acne_0\n",
      "Extracted: Acne_1.zip → Extracted\\Acne_1\n",
      "Extracted: Eczema_0.zip → Extracted\\Eczema_0\n",
      "Extracted: Eczema_1.zip → Extracted\\Eczema_1\n",
      "Extracted: Melasma_0.zip → Extracted\\Melasma_0\n",
      "Extracted: Melasma_1.zip → Extracted\\Melasma_1\n",
      "Extracted: Melasma_2.zip → Extracted\\Melasma_2\n",
      "Extracted: Melasma_3.zip → Extracted\\Melasma_3\n",
      "Extracted: Rosacea_0.zip → Extracted\\Rosacea_0\n",
      "Extracted: Shingles_0.zip → Extracted\\Shingles_0\n",
      "Extracted: Shingles_1.zip → Extracted\\Shingles_1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "\n",
    "# Define the base directory where ZIP files are located\n",
    "base_dir = \"RawDatasets\"\n",
    "\n",
    "# Extracted folder will be outside the base_dir\n",
    "extract_dir = os.path.join(os.path.dirname(base_dir), \"Extracted\")\n",
    "\n",
    "# Ensure the extracted directory exists\n",
    "os.makedirs(extract_dir, exist_ok=True)\n",
    "\n",
    "# Loop through all folders and extract ZIP files separately\n",
    "for root, _, files in os.walk(base_dir):\n",
    "    for file in files:\n",
    "        if file.endswith(\".zip\"):\n",
    "            zip_path = os.path.join(root, file)\n",
    "            folder_name = os.path.splitext(file)[0]  # Get ZIP filename without extension\n",
    "            target_dir = os.path.join(extract_dir, folder_name)  # Create a separate folder\n",
    "            os.makedirs(target_dir, exist_ok=True)  # Ensure folder exists\n",
    "\n",
    "            with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "                zip_ref.extractall(target_dir)  # Extract into separate folder\n",
    "            print(f\"Extracted: {file} → {target_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678fe948",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#f0f0f0; padding:20px; border-radius:12px; max-width:1490px; width:100%; box-sizing:border-box;\">\n",
    "\n",
    "  <h1 style=\"margin:0; color:#2c3e50; font-size:32px;\">Scan</h1>\n",
    "\n",
    "  <p style=\"margin:10px 0 0 0; font-size:16px; color:#34495e;\">\n",
    "    The <strong>Scan</strong> step displays the contents of a dataset folder in a \n",
    "    hierarchical tree view. This provides a clear overview of the directory \n",
    "    structure, including subfolders and files, making it easier to verify \n",
    "    dataset organization before cleaning or preprocessing.\n",
    "  </p>\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3230dfb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m📁 Scanning folder hierarchy in:\u001b[0m \u001b[96mExtracted\u001b[0m\n",
      "\n",
      "\u001b[96m|-- 📂 \u001b[1mAcne_0\u001b[0m ✅ 2513 images ✅ 2513 labels\n",
      "\u001b[92m    |-- 📄 README.dataset.txt\u001b[0m\n",
      "\u001b[92m    |-- 📄 README.roboflow.txt\u001b[0m\n",
      "\u001b[92m    |-- 📄 data.yaml\u001b[0m\n",
      "\u001b[96m    |-- 📂 \u001b[1mtest\u001b[0m \n",
      "\u001b[96m        |-- 📂 \u001b[1mimages\u001b[0m (130 files)\n",
      "\u001b[96m        |-- 📂 \u001b[1mlabels\u001b[0m (130 files)\n",
      "\u001b[96m    |-- 📂 \u001b[1mtrain\u001b[0m \n",
      "\u001b[96m        |-- 📂 \u001b[1mimages\u001b[0m (2238 files)\n",
      "\u001b[96m        |-- 📂 \u001b[1mlabels\u001b[0m (2238 files)\n",
      "\u001b[96m    |-- 📂 \u001b[1mvalid\u001b[0m \n",
      "\u001b[96m        |-- 📂 \u001b[1mimages\u001b[0m (145 files)\n",
      "\u001b[96m        |-- 📂 \u001b[1mlabels\u001b[0m (145 files)\n",
      "\u001b[96m|-- 📂 \u001b[1mAcne_1\u001b[0m ✅ 1282 images ✅ 1282 labels\n",
      "\u001b[92m    |-- 📄 README.dataset.txt\u001b[0m\n",
      "\u001b[92m    |-- 📄 README.roboflow.txt\u001b[0m\n",
      "\u001b[92m    |-- 📄 data.yaml\u001b[0m\n",
      "\u001b[96m    |-- 📂 \u001b[1mtest\u001b[0m \n",
      "\u001b[96m        |-- 📂 \u001b[1mimages\u001b[0m (18 files)\n",
      "\u001b[96m        |-- 📂 \u001b[1mlabels\u001b[0m (18 files)\n",
      "\u001b[96m    |-- 📂 \u001b[1mtrain\u001b[0m \n",
      "\u001b[96m        |-- 📂 \u001b[1mimages\u001b[0m (964 files)\n",
      "\u001b[96m        |-- 📂 \u001b[1mlabels\u001b[0m (964 files)\n",
      "\u001b[96m    |-- 📂 \u001b[1mvalid\u001b[0m \n",
      "\u001b[96m        |-- 📂 \u001b[1mimages\u001b[0m (300 files)\n",
      "\u001b[96m        |-- 📂 \u001b[1mlabels\u001b[0m (300 files)\n",
      "\u001b[96m|-- 📂 \u001b[1mEczema_0\u001b[0m ✅ 10152 images ✅ 10152 labels\n",
      "\u001b[92m    |-- 📄 README.dataset.txt\u001b[0m\n",
      "\u001b[92m    |-- 📄 README.roboflow.txt\u001b[0m\n",
      "\u001b[92m    |-- 📄 data.yaml\u001b[0m\n",
      "\u001b[96m    |-- 📂 \u001b[1mtest\u001b[0m \n",
      "\u001b[96m        |-- 📂 \u001b[1mimages\u001b[0m (70 files)\n",
      "\u001b[96m        |-- 📂 \u001b[1mlabels\u001b[0m (70 files)\n",
      "\u001b[96m    |-- 📂 \u001b[1mtrain\u001b[0m \n",
      "\u001b[96m        |-- 📂 \u001b[1mimages\u001b[0m (9225 files)\n",
      "\u001b[96m        |-- 📂 \u001b[1mlabels\u001b[0m (9225 files)\n",
      "\u001b[96m    |-- 📂 \u001b[1mvalid\u001b[0m \n",
      "\u001b[96m        |-- 📂 \u001b[1mimages\u001b[0m (857 files)\n",
      "\u001b[96m        |-- 📂 \u001b[1mlabels\u001b[0m (857 files)\n",
      "\u001b[96m|-- 📂 \u001b[1mEczema_1\u001b[0m ✅ 1512 images ✅ 1512 labels\n",
      "\u001b[92m    |-- 📄 README.dataset.txt\u001b[0m\n",
      "\u001b[92m    |-- 📄 README.roboflow.txt\u001b[0m\n",
      "\u001b[92m    |-- 📄 data.yaml\u001b[0m\n",
      "\u001b[96m    |-- 📂 \u001b[1mtest\u001b[0m \n",
      "\u001b[96m        |-- 📂 \u001b[1mimages\u001b[0m (158 files)\n",
      "\u001b[96m        |-- 📂 \u001b[1mlabels\u001b[0m (158 files)\n",
      "\u001b[96m    |-- 📂 \u001b[1mtrain\u001b[0m \n",
      "\u001b[96m        |-- 📂 \u001b[1mimages\u001b[0m (1054 files)\n",
      "\u001b[96m        |-- 📂 \u001b[1mlabels\u001b[0m (1054 files)\n",
      "\u001b[96m    |-- 📂 \u001b[1mvalid\u001b[0m \n",
      "\u001b[96m        |-- 📂 \u001b[1mimages\u001b[0m (300 files)\n",
      "\u001b[96m        |-- 📂 \u001b[1mlabels\u001b[0m (300 files)\n",
      "\u001b[96m|-- 📂 \u001b[1mMelasma_0\u001b[0m ✅ 223 images ✅ 223 labels\n",
      "\u001b[92m    |-- 📄 README.dataset.txt\u001b[0m\n",
      "\u001b[92m    |-- 📄 README.roboflow.txt\u001b[0m\n",
      "\u001b[92m    |-- 📄 data.yaml\u001b[0m\n",
      "\u001b[96m    |-- 📂 \u001b[1mtest\u001b[0m \n",
      "\u001b[96m        |-- 📂 \u001b[1mimages\u001b[0m (22 files)\n",
      "\u001b[96m        |-- 📂 \u001b[1mlabels\u001b[0m (22 files)\n",
      "\u001b[96m    |-- 📂 \u001b[1mtrain\u001b[0m \n",
      "\u001b[96m        |-- 📂 \u001b[1mimages\u001b[0m (180 files)\n",
      "\u001b[96m        |-- 📂 \u001b[1mlabels\u001b[0m (180 files)\n",
      "\u001b[96m    |-- 📂 \u001b[1mvalid\u001b[0m \n",
      "\u001b[96m        |-- 📂 \u001b[1mimages\u001b[0m (21 files)\n",
      "\u001b[96m        |-- 📂 \u001b[1mlabels\u001b[0m (21 files)\n",
      "\u001b[96m|-- 📂 \u001b[1mMelasma_1\u001b[0m ✅ 308 images ✅ 308 labels\n",
      "\u001b[92m    |-- 📄 README.dataset.txt\u001b[0m\n",
      "\u001b[92m    |-- 📄 README.roboflow.txt\u001b[0m\n",
      "\u001b[92m    |-- 📄 data.yaml\u001b[0m\n",
      "\u001b[96m    |-- 📂 \u001b[1mtest\u001b[0m \n",
      "\u001b[96m        |-- 📂 \u001b[1mimages\u001b[0m (31 files)\n",
      "\u001b[96m        |-- 📂 \u001b[1mlabels\u001b[0m (31 files)\n",
      "\u001b[96m    |-- 📂 \u001b[1mtrain\u001b[0m \n",
      "\u001b[96m        |-- 📂 \u001b[1mimages\u001b[0m (215 files)\n",
      "\u001b[96m        |-- 📂 \u001b[1mlabels\u001b[0m (215 files)\n",
      "\u001b[96m    |-- 📂 \u001b[1mvalid\u001b[0m \n",
      "\u001b[96m        |-- 📂 \u001b[1mimages\u001b[0m (62 files)\n",
      "\u001b[96m        |-- 📂 \u001b[1mlabels\u001b[0m (62 files)\n",
      "\u001b[96m|-- 📂 \u001b[1mMelasma_2\u001b[0m ✅ 69 images ✅ 69 labels\n",
      "\u001b[92m    |-- 📄 README.dataset.txt\u001b[0m\n",
      "\u001b[92m    |-- 📄 README.roboflow.txt\u001b[0m\n",
      "\u001b[92m    |-- 📄 data.yaml\u001b[0m\n",
      "\u001b[96m    |-- 📂 \u001b[1mtest\u001b[0m \n",
      "\u001b[96m        |-- 📂 \u001b[1mimages\u001b[0m (8 files)\n",
      "\u001b[96m        |-- 📂 \u001b[1mlabels\u001b[0m (8 files)\n",
      "\u001b[96m    |-- 📂 \u001b[1mtrain\u001b[0m \n",
      "\u001b[96m        |-- 📂 \u001b[1mimages\u001b[0m (47 files)\n",
      "\u001b[96m        |-- 📂 \u001b[1mlabels\u001b[0m (47 files)\n",
      "\u001b[96m    |-- 📂 \u001b[1mvalid\u001b[0m \n",
      "\u001b[96m        |-- 📂 \u001b[1mimages\u001b[0m (14 files)\n",
      "\u001b[96m        |-- 📂 \u001b[1mlabels\u001b[0m (14 files)\n",
      "\u001b[96m|-- 📂 \u001b[1mMelasma_3\u001b[0m ✅ 308 images ✅ 308 labels\n",
      "\u001b[92m    |-- 📄 README.dataset.txt\u001b[0m\n",
      "\u001b[92m    |-- 📄 README.roboflow.txt\u001b[0m\n",
      "\u001b[92m    |-- 📄 data.yaml\u001b[0m\n",
      "\u001b[96m    |-- 📂 \u001b[1mtest\u001b[0m \n",
      "\u001b[96m        |-- 📂 \u001b[1mimages\u001b[0m (31 files)\n",
      "\u001b[96m        |-- 📂 \u001b[1mlabels\u001b[0m (31 files)\n",
      "\u001b[96m    |-- 📂 \u001b[1mtrain\u001b[0m \n",
      "\u001b[96m        |-- 📂 \u001b[1mimages\u001b[0m (215 files)\n",
      "\u001b[96m        |-- 📂 \u001b[1mlabels\u001b[0m (215 files)\n",
      "\u001b[96m    |-- 📂 \u001b[1mvalid\u001b[0m \n",
      "\u001b[96m        |-- 📂 \u001b[1mimages\u001b[0m (62 files)\n",
      "\u001b[96m        |-- 📂 \u001b[1mlabels\u001b[0m (62 files)\n",
      "\u001b[96m|-- 📂 \u001b[1mRosacea_0\u001b[0m ✅ 5037 images ✅ 5037 labels\n",
      "\u001b[92m    |-- 📄 README.dataset.txt\u001b[0m\n",
      "\u001b[92m    |-- 📄 README.roboflow.txt\u001b[0m\n",
      "\u001b[92m    |-- 📄 data.yaml\u001b[0m\n",
      "\u001b[96m    |-- 📂 \u001b[1mtest\u001b[0m \n",
      "\u001b[96m        |-- 📂 \u001b[1mimages\u001b[0m (304 files)\n",
      "\u001b[96m        |-- 📂 \u001b[1mlabels\u001b[0m (304 files)\n",
      "\u001b[96m    |-- 📂 \u001b[1mtrain\u001b[0m \n",
      "\u001b[96m        |-- 📂 \u001b[1mimages\u001b[0m (4202 files)\n",
      "\u001b[96m        |-- 📂 \u001b[1mlabels\u001b[0m (4202 files)\n",
      "\u001b[96m    |-- 📂 \u001b[1mvalid\u001b[0m \n",
      "\u001b[96m        |-- 📂 \u001b[1mimages\u001b[0m (531 files)\n",
      "\u001b[96m        |-- 📂 \u001b[1mlabels\u001b[0m (531 files)\n",
      "\u001b[96m|-- 📂 \u001b[1mShingles_0\u001b[0m ✅ 2054 images ✅ 2054 labels\n",
      "\u001b[92m    |-- 📄 README.dataset.txt\u001b[0m\n",
      "\u001b[92m    |-- 📄 README.roboflow.txt\u001b[0m\n",
      "\u001b[92m    |-- 📄 data.yaml\u001b[0m\n",
      "\u001b[96m    |-- 📂 \u001b[1mtest\u001b[0m \n",
      "\u001b[96m        |-- 📂 \u001b[1mimages\u001b[0m (110 files)\n",
      "\u001b[96m        |-- 📂 \u001b[1mlabels\u001b[0m (110 files)\n",
      "\u001b[96m    |-- 📂 \u001b[1mtrain\u001b[0m \n",
      "\u001b[96m        |-- 📂 \u001b[1mimages\u001b[0m (1755 files)\n",
      "\u001b[96m        |-- 📂 \u001b[1mlabels\u001b[0m (1755 files)\n",
      "\u001b[96m    |-- 📂 \u001b[1mvalid\u001b[0m \n",
      "\u001b[96m        |-- 📂 \u001b[1mimages\u001b[0m (189 files)\n",
      "\u001b[96m        |-- 📂 \u001b[1mlabels\u001b[0m (189 files)\n",
      "\u001b[96m|-- 📂 \u001b[1mShingles_1\u001b[0m ✅ 12992 images ✅ 12992 labels\n",
      "\u001b[92m    |-- 📄 README.dataset.txt\u001b[0m\n",
      "\u001b[92m    |-- 📄 README.roboflow.txt\u001b[0m\n",
      "\u001b[92m    |-- 📄 data.yaml\u001b[0m\n",
      "\u001b[96m    |-- 📂 \u001b[1mtest\u001b[0m \n",
      "\u001b[96m        |-- 📂 \u001b[1mimages\u001b[0m (500 files)\n",
      "\u001b[96m        |-- 📂 \u001b[1mlabels\u001b[0m (500 files)\n",
      "\u001b[96m    |-- 📂 \u001b[1mtrain\u001b[0m \n",
      "\u001b[96m        |-- 📂 \u001b[1mimages\u001b[0m (11988 files)\n",
      "\u001b[96m        |-- 📂 \u001b[1mlabels\u001b[0m (11988 files)\n",
      "\u001b[96m    |-- 📂 \u001b[1mvalid\u001b[0m \n",
      "\u001b[96m        |-- 📂 \u001b[1mimages\u001b[0m (504 files)\n",
      "\u001b[96m        |-- 📂 \u001b[1mlabels\u001b[0m (504 files)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define the root directory to scan\n",
    "root_dir = \"Extracted\"\n",
    "\n",
    "# ANSI escape codes for colors\n",
    "CYAN = \"\\033[96m\"\n",
    "YELLOW = \"\\033[93m\"\n",
    "GREEN = \"\\033[92m\"\n",
    "BOLD = \"\\033[1m\"\n",
    "RESET = \"\\033[0m\"\n",
    "\n",
    "# Function to count files in a folder\n",
    "def count_files(directory):\n",
    "    return len([f for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))])\n",
    "\n",
    "# Function to count total images and labels in a dataset folder\n",
    "def count_dataset_items(dataset_path):\n",
    "    total_images, total_labels = 0, 0\n",
    "    for split in [\"train\", \"valid\", \"test\"]:\n",
    "        split_path = os.path.join(dataset_path, split)\n",
    "        images_path = os.path.join(split_path, \"images\")\n",
    "        labels_path = os.path.join(split_path, \"labels\")\n",
    "\n",
    "        if os.path.isdir(images_path):\n",
    "            total_images += count_files(images_path)\n",
    "        if os.path.isdir(labels_path):\n",
    "            total_labels += count_files(labels_path)\n",
    "\n",
    "    return total_images, total_labels\n",
    "\n",
    "# Function to recursively scan and display folder hierarchy with image/label counts\n",
    "def scan_folders(directory, indent=0):\n",
    "    if not os.path.exists(directory):\n",
    "        print(f\"{YELLOW}⚠️ Directory not found!{RESET}\")\n",
    "        return\n",
    "\n",
    "    for item in sorted(os.listdir(directory)):  # Sort for consistent order\n",
    "        item_path = os.path.join(directory, item)\n",
    "        prefix = \" \" * indent + \"|-- \"  # Indentation for hierarchy visualization\n",
    "        \n",
    "        if os.path.isdir(item_path):\n",
    "            if item in [\"images\", \"labels\"]:  # Count files instead of listing them\n",
    "                file_count = count_files(item_path)\n",
    "                print(f\"{CYAN}{prefix}📂 {BOLD}{item}{RESET} ({file_count} files)\")\n",
    "            else:\n",
    "                # Count total images and labels for dataset folders\n",
    "                total_images, total_labels = count_dataset_items(item_path)\n",
    "                \n",
    "                # Format the display string (hide 0 values)\n",
    "                count_text = []\n",
    "                if total_images > 0:\n",
    "                    count_text.append(f\"✅ {total_images} images\")\n",
    "                if total_labels > 0:\n",
    "                    count_text.append(f\"✅ {total_labels} labels\")\n",
    "                \n",
    "                count_display = \" \".join(count_text) if count_text else \"\"  \n",
    "                print(f\"{CYAN}{prefix}📂 {BOLD}{item}{RESET} {count_display}\")\n",
    "                \n",
    "                scan_folders(item_path, indent + 4)  # Recursively scan subfolders\n",
    "        else:\n",
    "            print(f\"{GREEN}{prefix}📄 {item}{RESET}\")\n",
    "\n",
    "# Run the folder scan\n",
    "print(f\"\\n{BOLD}📁 Scanning folder hierarchy in:{RESET} {CYAN}{root_dir}{RESET}\\n\")\n",
    "scan_folders(root_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9bdd48",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#f0f0f0; padding:20px; border-radius:12px; max-width:1490px; width:100%; box-sizing:border-box;\">\n",
    "\n",
    "  <h1 style=\"margin:0; color:#2c3e50; font-size:32px;\">2. Remove augmented Images</h1>\n",
    "\n",
    "  <p style=\"margin:10px 0 0 0; font-size:16px; color:#34495e;\">\n",
    "    You will still need to manually verify it.\n",
    "  </p>\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff21a4e",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#f0f0f0; padding:20px; border-radius:12px; max-width:1490px; width:100%; box-sizing:border-box;\">\n",
    "\n",
    "  <h1 style=\"margin:0; color:#2c3e50; font-size:32px;\">Scan all duplicates (Augmented)</h1>\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4820f034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[36m\n",
      "📁 Scanning for duplicate images in datasets inside: Extracted\n",
      "\u001b[0m\n",
      "\u001b[32m📂 Acne_0 → ✅ No duplicate images\u001b[0m\n",
      "\u001b[32m📂 Acne_1 → ✅ No duplicate images\u001b[0m\n",
      "\u001b[32m📂 Eczema_0 → ✅ No duplicate images\u001b[0m\n",
      "\u001b[32m📂 Eczema_1 → ✅ No duplicate images\u001b[0m\n",
      "\u001b[32m📂 Melasma_0 → ✅ No duplicate images\u001b[0m\n",
      "\u001b[32m📂 Melasma_1 → ✅ No duplicate images\u001b[0m\n",
      "\u001b[32m📂 Melasma_2 → ✅ No duplicate images\u001b[0m\n",
      "\u001b[32m📂 Melasma_3 → ✅ No duplicate images\u001b[0m\n",
      "\u001b[32m📂 Rosacea_0 → ✅ No duplicate images\u001b[0m\n",
      "\u001b[32m📂 Shingles_0 → ✅ No duplicate images\u001b[0m\n",
      "\u001b[32m📂 Shingles_1 → ✅ No duplicate images\u001b[0m\n",
      "\u001b[1m\u001b[35m\n",
      "🧮 Grand Total Duplicate Images Across All Datasets: 0\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "from termcolor import colored\n",
    "\n",
    "# Define root path where all datasets are located\n",
    "root_path = r'Extracted'\n",
    "folders = ['train', 'valid', 'test']\n",
    "image_exts = ('.jpg', '.png', '.jpeg')\n",
    "\n",
    "# Grand total for all datasets\n",
    "grand_total_duplicates = 0\n",
    "\n",
    "print(colored(f\"\\n📁 Scanning for duplicate images in datasets inside: {root_path}\\n\", \"cyan\", attrs=[\"bold\"]))\n",
    "\n",
    "# Iterate over each dataset folder (e.g., Acne_0, Eczema_1)\n",
    "for dataset_name in sorted(os.listdir(root_path)):\n",
    "    dataset_path = os.path.join(root_path, dataset_name)\n",
    "    if not os.path.isdir(dataset_path):\n",
    "        continue\n",
    "\n",
    "    total_duplicates = 0\n",
    "\n",
    "    for folder in folders:\n",
    "        image_folder = os.path.join(dataset_path, folder, 'images')\n",
    "\n",
    "        if not os.path.exists(image_folder):\n",
    "            continue\n",
    "\n",
    "        filename_dict = defaultdict(int)\n",
    "\n",
    "        # Count image base names (e.g., Acne_001 from Acne_001.rf.asdf123.jpg)\n",
    "        for filename in os.listdir(image_folder):\n",
    "            if filename.lower().endswith(image_exts):\n",
    "                base_name = filename.rsplit('_', 1)[0]\n",
    "                filename_dict[base_name] += 1\n",
    "\n",
    "        # Count duplicates\n",
    "        for count in filename_dict.values():\n",
    "            if count > 1:\n",
    "                total_duplicates += count - 1\n",
    "\n",
    "    if total_duplicates > 0:\n",
    "        print(colored(f\"📂 {dataset_name} → ❌ {total_duplicates} duplicate images found\", \"red\"))\n",
    "    else:\n",
    "        print(colored(f\"📂 {dataset_name} → ✅ No duplicate images\", \"green\"))\n",
    "\n",
    "    grand_total_duplicates += total_duplicates\n",
    "\n",
    "# Final summary\n",
    "print(colored(f\"\\n🧮 Grand Total Duplicate Images Across All Datasets: {grand_total_duplicates}\\n\", \"magenta\", attrs=[\"bold\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1c0a6c",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#f0f0f0; padding:20px; border-radius:12px; max-width:1490px; width:100%; box-sizing:border-box;\">\n",
    "\n",
    "  <h1 style=\"margin:0; color:#2c3e50; font-size:32px;\">2. Remove Augmented Images</h1>\n",
    "\n",
    "  <p style=\"margin:10px 0 0 0; font-size:16px; color:#34495e;\">\n",
    "    The <strong>code</strong> below removes all augmented files in the \"Extracted\" folder and place them in a new folder called \"Cleaned\".\n",
    "  </p>\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e263361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📁 Duplicate Removal Summary Across All Datasets:\n",
      "\n",
      "📂 Acne_0 → ❌ 1944 duplicate images removed\n",
      "📂 Acne_1 → ❌ 674 duplicate images removed\n",
      "📂 Eczema_0 → ❌ 6261 duplicate images removed\n",
      "📂 Eczema_1 → ✅ No duplicates found\n",
      "📂 Melasma_0 → ❌ 101 duplicate images removed\n",
      "📂 Melasma_1 → ✅ No duplicates found\n",
      "📂 Melasma_2 → ✅ No duplicates found\n",
      "📂 Melasma_3 → ✅ No duplicates found\n",
      "📂 Rosacea_0 → ❌ 2200 duplicate images removed\n",
      "📂 Shingles_0 → ❌ 1181 duplicate images removed\n",
      "📂 Shingles_1 → ❌ 7992 duplicate images removed\n",
      "\n",
      "🧮 Grand Total Duplicates Removed: 20353\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from collections import defaultdict\n",
    "\n",
    "# Main paths\n",
    "base_path = r'Extracted'\n",
    "cleaned_base_path = r'Cleaned'\n",
    "folders = ['train', 'valid', 'test']\n",
    "image_exts = ('.jpg', '.jpeg', '.png')\n",
    "\n",
    "# Function to get base filename (excluding hash)\n",
    "def get_base_filename(filename):\n",
    "    return filename.rsplit('_', 1)[0]\n",
    "\n",
    "# Global counters\n",
    "grand_total_duplicates = 0\n",
    "summary = {}\n",
    "\n",
    "# Process each dataset inside \"extracted\"\n",
    "for dataset in os.listdir(base_path):\n",
    "    dataset_path = os.path.join(base_path, dataset)\n",
    "    if not os.path.isdir(dataset_path):\n",
    "        continue\n",
    "\n",
    "    cleaned_path = os.path.join(cleaned_base_path, dataset)\n",
    "    \n",
    "    # Prepare cleaned folders\n",
    "    for folder in folders:\n",
    "        os.makedirs(os.path.join(cleaned_path, folder, 'images'), exist_ok=True)\n",
    "        os.makedirs(os.path.join(cleaned_path, folder, 'labels'), exist_ok=True)\n",
    "    \n",
    "    # Registry to store all base names in this dataset\n",
    "    image_registry = defaultdict(list)\n",
    "    duplicates_count = {folder: 0 for folder in folders}\n",
    "\n",
    "    # Step 1: Register image filenames\n",
    "    for folder in folders:\n",
    "        image_folder = os.path.join(dataset_path, folder, 'images')\n",
    "        if not os.path.exists(image_folder):\n",
    "            continue\n",
    "        for filename in os.listdir(image_folder):\n",
    "            if not filename.lower().endswith(image_exts):\n",
    "                continue\n",
    "            base_name = get_base_filename(filename)\n",
    "            image_registry[(folder, base_name)].append(filename)\n",
    "\n",
    "    # Step 2: Remove duplicates and move them to cleaned folder\n",
    "    for (folder, base_name), filenames in image_registry.items():\n",
    "        if len(filenames) > 1:\n",
    "            duplicates_count[folder] += len(filenames) - 1\n",
    "            image_folder = os.path.join(dataset_path, folder, 'images')\n",
    "            label_folder = os.path.join(dataset_path, folder, 'labels')\n",
    "            cleaned_image_folder = os.path.join(cleaned_path, folder, 'images')\n",
    "            cleaned_label_folder = os.path.join(cleaned_path, folder, 'labels')\n",
    "            for file in filenames[1:]:\n",
    "                # Move image\n",
    "                src_image = os.path.join(image_folder, file)\n",
    "                dst_image = os.path.join(cleaned_image_folder, file)\n",
    "                if os.path.exists(src_image):\n",
    "                    shutil.move(src_image, dst_image)\n",
    "\n",
    "                # Move corresponding label\n",
    "                label_file = os.path.splitext(file)[0] + '.txt'\n",
    "                src_label = os.path.join(label_folder, label_file)\n",
    "                dst_label = os.path.join(cleaned_label_folder, label_file)\n",
    "                if os.path.exists(src_label):\n",
    "                    shutil.move(src_label, dst_label)\n",
    "\n",
    "    # Collect summary\n",
    "    dataset_total = sum(duplicates_count.values())\n",
    "    grand_total_duplicates += dataset_total\n",
    "    summary[dataset] = dataset_total\n",
    "\n",
    "# Final Output\n",
    "print(\"\\n📁 Duplicate Removal Summary Across All Datasets:\\n\")\n",
    "for dataset, count in summary.items():\n",
    "    if count > 0:\n",
    "        print(f\"📂 {dataset} → ❌ {count} duplicate images removed\")\n",
    "    else:\n",
    "        print(f\"📂 {dataset} → ✅ No duplicates found\")\n",
    "print(f\"\\n🧮 Grand Total Duplicates Removed: {grand_total_duplicates}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "da05a9b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory hierarchy for: C:\\Users\\rapha\\OneDrive\\Desktop\\CS Thesis 2\n",
      "\n",
      "    📄 .gitignore\n",
      "    📄 main.ipynb\n",
      "    📄 README.md\n",
      "    📄 RunProjectThisDirectory.txt\n",
      "    📦 Cleaned/\n",
      "        📦 Acne_0/\n",
      "            📦 test/\n",
      "            📦 train/\n",
      "            📦 valid/\n",
      "        📦 Acne_1/\n",
      "            📦 test/\n",
      "            📦 train/\n",
      "            📦 valid/\n",
      "        📦 Eczema_0/\n",
      "            📦 test/\n",
      "            📦 train/\n",
      "            📦 valid/\n",
      "        📦 Eczema_1/\n",
      "            📦 test/\n",
      "            📦 train/\n",
      "            📦 valid/\n",
      "        📦 Melasma_0/\n",
      "            📦 test/\n",
      "            📦 train/\n",
      "            📦 valid/\n",
      "        📦 Melasma_1/\n",
      "            📦 test/\n",
      "            📦 train/\n",
      "            📦 valid/\n",
      "        📦 Melasma_2/\n",
      "            📦 test/\n",
      "            📦 train/\n",
      "            📦 valid/\n",
      "        📦 Melasma_3/\n",
      "            📦 test/\n",
      "            📦 train/\n",
      "            📦 valid/\n",
      "        📦 Rosacea_0/\n",
      "            📦 test/\n",
      "            📦 train/\n",
      "            📦 valid/\n",
      "        📦 Shingles_0/\n",
      "            📦 test/\n",
      "            📦 train/\n",
      "            📦 valid/\n",
      "        📦 Shingles_1/\n",
      "            📦 test/\n",
      "            📦 train/\n",
      "            📦 valid/\n",
      "    📦 Extracted/\n",
      "        📦 Acne_0/\n",
      "            📄 data.yaml\n",
      "            📄 README.dataset.txt\n",
      "            📄 README.roboflow.txt\n",
      "            📦 test/\n",
      "            📦 train/\n",
      "            📦 valid/\n",
      "        📦 Acne_1/\n",
      "            📄 data.yaml\n",
      "            📄 README.dataset.txt\n",
      "            📄 README.roboflow.txt\n",
      "            📦 test/\n",
      "            📦 train/\n",
      "            📦 valid/\n",
      "        📦 Eczema_0/\n",
      "            📄 data.yaml\n",
      "            📄 README.dataset.txt\n",
      "            📄 README.roboflow.txt\n",
      "            📦 test/\n",
      "            📦 train/\n",
      "            📦 valid/\n",
      "        📦 Eczema_1/\n",
      "            📄 data.yaml\n",
      "            📄 README.dataset.txt\n",
      "            📄 README.roboflow.txt\n",
      "            📦 test/\n",
      "            📦 train/\n",
      "            📦 valid/\n",
      "        📦 Melasma_0/\n",
      "            📄 data.yaml\n",
      "            📄 README.dataset.txt\n",
      "            📄 README.roboflow.txt\n",
      "            📦 test/\n",
      "            📦 train/\n",
      "            📦 valid/\n",
      "        📦 Melasma_1/\n",
      "            📄 data.yaml\n",
      "            📄 README.dataset.txt\n",
      "            📄 README.roboflow.txt\n",
      "            📦 test/\n",
      "            📦 train/\n",
      "            📦 valid/\n",
      "        📦 Melasma_2/\n",
      "            📄 data.yaml\n",
      "            📄 README.dataset.txt\n",
      "            📄 README.roboflow.txt\n",
      "            📦 test/\n",
      "            📦 train/\n",
      "            📦 valid/\n",
      "        📦 Melasma_3/\n",
      "            📄 data.yaml\n",
      "            📄 README.dataset.txt\n",
      "            📄 README.roboflow.txt\n",
      "            📦 test/\n",
      "            📦 train/\n",
      "            📦 valid/\n",
      "        📦 Rosacea_0/\n",
      "            📄 data.yaml\n",
      "            📄 README.dataset.txt\n",
      "            📄 README.roboflow.txt\n",
      "            📦 test/\n",
      "            📦 train/\n",
      "            📦 valid/\n",
      "        📦 Shingles_0/\n",
      "            📄 data.yaml\n",
      "            📄 README.dataset.txt\n",
      "            📄 README.roboflow.txt\n",
      "            📦 test/\n",
      "            📦 train/\n",
      "            📦 valid/\n",
      "        📦 Shingles_1/\n",
      "            📄 data.yaml\n",
      "            📄 README.dataset.txt\n",
      "            📄 README.roboflow.txt\n",
      "            📦 test/\n",
      "            📦 train/\n",
      "            📦 valid/\n",
      "    📦 RawDatasets/\n",
      "        📦 Acne/\n",
      "            📄 Acne_0.zip\n",
      "            📄 Acne_1.zip\n",
      "        📦 Eczema/\n",
      "            📄 Eczema_0.zip\n",
      "            📄 Eczema_1.zip\n",
      "        📦 Melasma/\n",
      "            📄 Melasma_0.zip\n",
      "            📄 Melasma_1.zip\n",
      "            📄 Melasma_2.zip\n",
      "            📄 Melasma_3.zip\n",
      "        📦 Rosecea/\n",
      "            📄 Rosacea_0.zip\n",
      "        📦 Shingles/\n",
      "            📄 Shingles_0.zip\n",
      "            📄 Shingles_1.zip\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def list_files_hierarchy(startpath):\n",
    "    \"\"\"\n",
    "    Displays the file and directory hierarchy from a given start path,\n",
    "    excluding specified directories like node_modules.\n",
    "    \"\"\"\n",
    "    # Define directories to exclude from the hierarchy display\n",
    "    # You can add more directories here if needed (e.g., '.git', '.expo', '__pycache__')\n",
    "    excluded_dirs = ['images', 'labels', '.vscode', '.venv', '.git' ]\n",
    "\n",
    "    if not os.path.isdir(startpath):\n",
    "        print(f\"Error: Directory '{startpath}' not found.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Directory hierarchy for: {startpath}\\n\")\n",
    "\n",
    "    for root, dirs, files in os.walk(startpath):\n",
    "        # Calculate current level for indentation\n",
    "        level = root.replace(startpath, '').count(os.sep)\n",
    "        indent = ' ' * 4 * (level)\n",
    "\n",
    "        # Print current directory\n",
    "        # The first root will be the startpath itself, handled below\n",
    "        if root != startpath:\n",
    "            dir_name = os.path.basename(root)\n",
    "            # Check if current directory should be excluded\n",
    "            if dir_name in excluded_dirs:\n",
    "                # If a directory is in excluded_dirs, we skip its contents\n",
    "                # and prevent os.walk from descending into it.\n",
    "                del dirs[:] # This modifies 'dirs' in-place, preventing os.walk from entering these.\n",
    "                continue\n",
    "            print(f'{indent}📦 {dir_name}/') # Folder icon\n",
    "\n",
    "        # Remove excluded directories from the list to prevent os.walk from entering them\n",
    "        # This is important to not process files inside excluded_dirs\n",
    "        dirs[:] = [d for d in dirs if d not in excluded_dirs]\n",
    "\n",
    "\n",
    "        subindent = ' ' * 4 * (level + 1)\n",
    "        for f in files:\n",
    "            print(f'{subindent}📄 {f}') # File icon\n",
    "\n",
    "# --- How to use it ---\n",
    "if __name__ == \"__main__\":\n",
    "    # You can set the path manually or use os.getcwd() to get the current working directory\n",
    "    project_root = r\"C:\\Users\\rapha\\OneDrive\\Desktop\\CS Thesis 2\" # Use 'r' for raw string to handle backslashes\n",
    "    # Or to use the directory where the script is run:\n",
    "    # project_root = os.getcwd()\n",
    "\n",
    "    list_files_hierarchy(project_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f608a0f",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#f0f0f0; padding:20px; border-radius:12px; max-width:1490px; width:100%; box-sizing:border-box;\">\n",
    "\n",
    "  <h1 style=\"margin:0; color:#2c3e50; font-size:32px;\">Merge</h1>\n",
    "\n",
    "  <p style=\"margin:10px 0 0 0; font-size:16px; color:#34495e;\">\n",
    "    The <strong>code</strong> below merges nested folders (test, train, valid) of each dataset into a single folder for each dataset didsregarding the label folders.\n",
    "  </p>\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5ba09b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Acne_0...\n",
      "Processing Acne_1...\n",
      "Processing Eczema_0...\n",
      "Processing Eczema_1...\n",
      "Processing Melasma_0...\n",
      "Processing Melasma_1...\n",
      "Processing Melasma_2...\n",
      "Processing Melasma_3...\n",
      "Processing Rosacea_0...\n",
      "Processing Shingles_0...\n",
      "Processing Shingles_1...\n",
      "\n",
      "✅ Merging complete! All images are in: Merged\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Base Extracted directory (relative path)\n",
    "base_dir = \"Extracted\"\n",
    "merged_dir = \"Merged\"\n",
    "\n",
    "# Make sure Merged folder exists\n",
    "os.makedirs(merged_dir, exist_ok=True)\n",
    "\n",
    "# Walk through each dataset folder inside Extracted\n",
    "for dataset in os.listdir(base_dir):\n",
    "    dataset_path = os.path.join(base_dir, dataset)\n",
    "    if os.path.isdir(dataset_path):\n",
    "        print(f\"Processing {dataset}...\")\n",
    "\n",
    "        # Create subfolder inside Merged for this dataset\n",
    "        dataset_merged_dir = os.path.join(merged_dir, dataset)\n",
    "        os.makedirs(dataset_merged_dir, exist_ok=True)\n",
    "\n",
    "        # Look into test, train, valid\n",
    "        for subset in [\"train\", \"test\", \"valid\"]:\n",
    "            subset_path = os.path.join(dataset_path, subset)\n",
    "            if os.path.exists(subset_path):\n",
    "                for root, _, files in os.walk(subset_path):\n",
    "                    for file in files:\n",
    "                        # Only copy image files\n",
    "                        if file.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "                            src_file = os.path.join(root, file)\n",
    "                            dst_file = os.path.join(dataset_merged_dir, file)\n",
    "\n",
    "                            # Handle duplicates by prefixing subset name\n",
    "                            if os.path.exists(dst_file):\n",
    "                                filename, ext = os.path.splitext(file)\n",
    "                                dst_file = os.path.join(dataset_merged_dir, f\"{subset}_{filename}{ext}\")\n",
    "\n",
    "                            shutil.copy2(src_file, dst_file)\n",
    "\n",
    "print(f\"\\n✅ Merging complete! All images are in: {merged_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68081b70",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#f0f0f0; padding:20px; border-radius:12px; max-width:1490px; width:100%; box-sizing:border-box;\">\n",
    "\n",
    "  <h1 style=\"margin:0; color:#2c3e50; font-size:32px;\">Finish</h1>\n",
    "\n",
    "  <p style=\"margin:10px 0 0 0; font-size:16px; color:#34495e;\">\n",
    "    Now we do some labelling. We will use the \"Merged\" folder and label it in the website\n",
    "  </p>\n",
    "\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
